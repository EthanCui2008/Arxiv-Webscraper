{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "q6Du69zZhOrw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import tarfile\n",
        "\n",
        "import io\n",
        "from io import BytesIO\n",
        "\n",
        "import re\n",
        "import os\n",
        "\n",
        "import json\n",
        "import fitz\n",
        "import ast\n",
        "\n",
        "import sqlite3\n",
        "import hashlib\n",
        "\n",
        "import time\n",
        "from datetime import datetime, timedelta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61Zq6Oo0KRqk",
        "outputId": "424aa8fd-172f-4c00-c7ca-155ca28f3d60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv('Key.env')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqvcXNP2Gxcx",
        "outputId": "f46747ca-89bb-41ae-c632-8f3cc10da882"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    #Creates a client class\n",
        "        api_key=os.environ.get(\"API_KEY\"),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SlT4NmgOnmpp"
      },
      "outputs": [],
      "source": [
        "def is_valid_python_code(code_str):\n",
        "    try:\n",
        "        ast.parse(code_str)\n",
        "        return True\n",
        "    except SyntaxError:\n",
        "        return False\n",
        "    \n",
        "def create_unique_hash(input_string, input_list):\n",
        "    import hashlib\n",
        "    list_string = str(input_list)\n",
        "    combined_input = input_string + list_string\n",
        "    hash_object = hashlib.sha256(combined_input.encode())\n",
        "    unique_hash = hash_object.hexdigest()\n",
        "    return int(unique_hash[:8], 16)  # Convert part of the hash to an integer\n",
        "\n",
        "def generate_weekly_spans(year):\n",
        "    start_date = datetime(year, 1, 1)\n",
        "    \n",
        "    weekly_spans = []\n",
        "    while start_date.year == year:\n",
        "        end_date = start_date + timedelta(days=6) \n",
        "        weekly_spans.append((start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')))\n",
        "        start_date += timedelta(days=7)\n",
        "    \n",
        "    return weekly_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "exvexagRFX9M"
      },
      "outputs": [],
      "source": [
        "debug = False\n",
        "def process_pdf(url):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(\"Request Failed!\")\n",
        "\n",
        "    # Open the PDF file with PyMuPDF directly from the in-memory bytes\n",
        "    pdf_document = fitz.open(stream=BytesIO(response.content), filetype='pdf')\n",
        "\n",
        "    # Extract text from each page and store it in a string\n",
        "    pdf_text = \"\"\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document[page_num]\n",
        "        pdf_text += page.get_text()\n",
        "\n",
        "    # Optionally, encode the string as UTF-8\n",
        "    pdf_text_utf8 = pdf_text.encode('utf-8').decode('utf-8')\n",
        "\n",
        "    # Close the PDF document\n",
        "    pdf_document.close()\n",
        "    return pdf_text_utf8\n",
        "\n",
        "def prompt_gpt(pdf, prompt_txt, word_start, word_cutoff, format_needed):\n",
        "    # No good solution, simply grabs the first few hundred characters and hopes all the authors are in that area, has already failed so far, let's improve on this later\n",
        "    intro = pdf[word_start:word_cutoff]\n",
        "    # Prompts gpt [possibly just hard code this solutoin, honestly just doing this for practice]\n",
        "\n",
        "    valid_syntax = False\n",
        "    loop_count = 0\n",
        "\n",
        "    while(not valid_syntax):\n",
        "        intro = pdf[word_start:word_cutoff]\n",
        "        loop_count += 1\n",
        "        if(loop_count == 3):\n",
        "          #To prevent an infinite loop\n",
        "          return False\n",
        "\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"Without any extra explanation text, \" + prompt_txt + \" \" + intro,\n",
        "                }\n",
        "            ],\n",
        "            model=\"gpt-4\",\n",
        "        )\n",
        "        if(debug):\n",
        "            print(chat_completion.choices[0].message.content)\n",
        "\n",
        "        valid_syntax = (is_valid_python_code(chat_completion.choices[0].message.content)) or (not format_needed)\n",
        "\n",
        "    #AST will automatically throw an error if GPT's response is deemed bad\n",
        "    \n",
        "    if(format_needed):\n",
        "        return ast.literal_eval(chat_completion.choices[0].message.content)\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "def grab_author_institutions(pdf):\n",
        "    return prompt_gpt(pdf, \"find all authors and corresponding instiutions, and return them in a dictionary, author key, insti\",0,1500, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unite the Code that pulls from the source with code that pushes it to the dump folder, also make it go on a week by week basis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Arxiv API interface URL\n",
        "base_url = 'http://export.arxiv.org/api/query'\n",
        "\n",
        "#Query Given\n",
        "search_query = 'all:machine learning'\n",
        "\n",
        "#Amount of Paper data to be dumped into each .json\n",
        "max_results = 50\n",
        "\n",
        "#Jump off point for searching, gets slow when this number gets large\n",
        "start_index = 0  \n",
        "\n",
        "#Artifical limit set for now, eventually it will scrape the whole of a query\n",
        "total_papers = 200\n",
        "papers_data = []\n",
        "\n",
        "while len(papers_data) < total_papers:\n",
        "\n",
        "    params = {\n",
        "        'search_query': search_query,\n",
        "        'start': start_index,\n",
        "        'max_results': max_results,\n",
        "        'sortBy': 'submittedDate',\n",
        "        'sortOrder': 'descending'\n",
        "    }\n",
        "    \n",
        "    response = requests.get(base_url, params=params)\n",
        "    \n",
        "    if response.status_code != 200:\n",
        "        raise Exception(\"Failed to retrieve data\" + response.status_code)\n",
        "\n",
        "    root = ET.fromstring(response.content)\n",
        "    \n",
        "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
        "        abs_link = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
        "        title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()\n",
        "        published = entry.find('{http://www.w3.org/2005/Atom}published').text\n",
        "        authors = entry.findall('{http://www.w3.org/2005/Atom}author')\n",
        "        author_list = [author.find('{http://www.w3.org/2005/Atom}name').text for author in authors]\n",
        "        \n",
        "        papers_data.append({\n",
        "            'id': int(abs_link[21:].replace('.', '').replace('v', '')),\n",
        "            'title': title,\n",
        "            'published': published,\n",
        "            'authors': author_list\n",
        "        })\n",
        "        \n",
        "        if len(papers_data) >= total_papers:\n",
        "            break\n",
        "\n",
        "    start_index += max_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "json.dump(papers_data, open(\"dump/\" + \"firstdump\", 'w'), indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "conn = sqlite3.connect('arxiv-db.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS papers (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    title TEXT,\n",
        "    published TEXT,\n",
        "    authors TEXT\n",
        ")\n",
        "''')\n",
        "\n",
        "with open(\"dump/firstdump\", 'r') as read:\n",
        "    data = json.load(read)\n",
        "\n",
        "for paper in papers_data:\n",
        "\n",
        "    entry['authors'] = ', '.join(entry['authors'])\n",
        "    \n",
        "    cursor.execute('''\n",
        "    INSERT INTO papers (id, title, published, authors)\n",
        "    VALUES (?, ?, ?, ?)\n",
        "    ''', (entry['id'], entry['title'], entry['published'], entry['authors']))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
